{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOrv9MAulJ9kP7BmISKWpC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kilos11/Michigan-State-university-Natural-Language-Processing-/blob/main/Chapter_2_A_Quick_Tour_of_Traditional_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Corpora, Tokens, and Types*\n",
        "\n",
        "All NLP methods, be they classic or modern, begin with a text dataset, also called a corpus (plural:\n",
        "corpora). A corpus usually contains raw text (in ASCII or UTF­8) and any metadata associated with the text. The raw text is a sequence of characters (bytes), but most times it is useful to group those characters into contiguous units called tokens. In English, tokens correspond to words and numeric sequences separated by white­space characters or punctuation.\n",
        "The metadata could be any auxiliary piece of information associated with the text, like identifiers, labels, and timestamps. In machine learning parlance, the text along with its metadata is called an instance or data point. The corpus ( igure 2­1), a collection of instances, is also known as a dataset.\n",
        "Given the heavy machine learning focus of this book, we freely interchange the terms corpus and dataset throughout."
      ],
      "metadata": {
        "id": "nDxF-wGLms_5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGuc_YJ4lLvP",
        "outputId": "7b941e23-e87f-49f4-e22f-8ff923228b95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mary', ',', 'do', 'n’t', 'slap', 'the', 'green', 'witch']\n"
          ]
        }
      ],
      "source": [
        "#Example 2­1. Tokenizing text\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Mary, don’t slap the green witch\"\n",
        "print([str(token) for token in nlp(text.lower())])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tweet=u\"Snow White and the Seven Degrees#MakeAMovieCold@midnight:­)\"\n",
        "tokenizer = TweetTokenizer()\n",
        "print(tokenizer.tokenize(tweet.lower()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M-wxtHWpG9f",
        "outputId": "656da8c1-3dbd-4963-dc2d-516ee3b2c510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['snow', 'white', 'and', 'the', 'seven', 'degrees', '#makeamoviecold', '@midnight', ':', '\\xad', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 2­3. Lemmatization: reducing words to their root forms"
      ],
      "metadata": {
        "id": "SCvGJlZoVKdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"he was running lately\")\n",
        "for token in doc:\n",
        "    print('{} ­­> {}'.format(token, token.lemma_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pVaP3C3VPOL",
        "outputId": "6a546a1d-adee-482d-a582-3d60ae638d2e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "he ­­> he\n",
            "was ­­> be\n",
            "running ­­> run\n",
            "lately ­­> lately\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorizing Words: POS Tagging We can extend the concept of labeling from documents to individual words or tokens. A common example of categorizing words is part­of­speech (POS) tagging, as demonstrated in  example 2­4"
      ],
      "metadata": {
        "id": "K7HcWEDzWoyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"Mary slapped the green witch.\")\n",
        "for token in doc:\n",
        "    print('{} ­ {}'.format(token, token.pos_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSrldFoxWzgO",
        "outputId": "25c51e46-f99e-43b8-8e73-ce854812b234"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mary ­ PROPN\n",
            "slapped ­ VERB\n",
            "the ­ DET\n",
            "green ­ ADJ\n",
            "witch ­ NOUN\n",
            ". ­ PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Categorizing Spans: Chunking and Named Entity Recognition\n",
        "\n",
        "Often, we need to label a span of text; that is, a contiguous multitoken boundary. For example, consider the sentence, “Mary slapped the green witch.” We might want to identify the noun phrases (NP) and verb phrases (VP) in it, as shown here:\n",
        "\n",
        "Example 2­5. Noun Phrase (NP) chunking"
      ],
      "metadata": {
        "id": "nr1hzKzvXs49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc  = nlp(u\"Mary slapped the green witch.\")\n",
        "for chunk in doc.noun_chunks:\n",
        "    print ('{} ­ {}'.format(chunk, chunk.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T23EUaXkX7Om",
        "outputId": "6e329a5e-6057-4f92-bab2-60aebe943d9d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mary ­ NP\n",
            "the green witch ­ NP\n"
          ]
        }
      ]
    }
  ]
}